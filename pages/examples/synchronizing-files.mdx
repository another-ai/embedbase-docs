import Image from 'next/image'


# Synchronizing your files with Embedbase

First, install the JS SDK:

```bash
npm i embedbase-js
```

With this snippet of code, you can sync your files with Embedbase.
This is a very similar codebase to [what we use to sync our documentation with Embedbase](https://github.com/different-ai/embedbase-docs/blob/main/scripts/sync.ts).

```ts
const glob = require("glob");
const fs = require("fs");
import { createClient, BatchAddDocument } from "embedbase-js"
import { splitText } from "embedbase-js/dist/main/split";

try {
    require("dotenv").config();
} catch (e) {
    console.log("No .env file found" + e);
}
// you can find the api key at https://app.embedbase.xyz
const apiKey = process.env.EMBEDBASE_API_KEY;

// this is using the hosted instance
const url = "https://api.embedbase.xyz"
const embedbase = createClient(url, apiKey)

// (optional) replace with your own url
const baseUrl = "https://foo.com/"

const sync = async () => {
    // read all files under pages/* with .mdx extension
    // for each file, read the content
    // feel free to change the extension to suit your needs
    const documents = glob.sync("pages/**/*.mdx").map((path) => ({
        // optional, but recommended, it can be used to show reference to the original file
        // when you are searching.
        path: baseUrl + path.replace(.mdx, ""),
        // content of the file
        data: fs.readFileSync(path, "utf-8")
    }));
    const chunks = []
    documents.map((document) =>
        splitText(document.data, { maxTokens: 500, chunkOverlap: 200 }, async ({ chunk, start, end }) => chunks.push({
            data: chunk,
            metadata: {
                path: document.path,
                start,
                end
            }
        }))
    )
    const datasetId = `your-dataset-id`

    console.log(`Syncing to ${datasetId} ${chunks.length} documents`);

    const batchSize = 100;
    // add to embedbase by batches of size 100
    return Promise.all(
        chunks.reduce((acc: BatchAddDocument[][], chunk, i) => {
            if (i % batchSize === 0) {
                acc.push(chunks.slice(i, i + batchSize));
            }
            return acc;
        }, []).map((chunk) => embedbase.dataset(datasetId).batchAdd(chunk))
    )
        .then((e) => e.flat())
        .then((e) => console.log(`Synced ${e.length} documents to ${datasetId}`))
        .catch(console.error);
}

sync();
```

Note that one of the best practices is to add "path" in metadata as an URL to the original file.
This way, when you are searching, you can see the reference to the original file.

Imagine when you are building a ChatGPT experience, it would allow the AI
to give references to the original file when answering questions.

<Image src="/search-with-references.png" width={800} height={500} />
